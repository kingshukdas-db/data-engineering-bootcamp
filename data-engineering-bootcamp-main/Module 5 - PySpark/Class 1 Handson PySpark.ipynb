{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcb4c3f5-9a3a-468f-b3ac-980ad9b1c961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Class 2 of PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec4edef0-137e-4e37-8017-be78b0d33f15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Start with SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b19f00ae-8ca4-4ebd-810d-6daa2c678022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "print(\"SparkSession Created :\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "845f6ed4-09c3-49ed-a92e-bf1dfa770c30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Understand What a DataFrame Is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "179acdac-b8e2-4b1c-a428-443beae541d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create Spark Dataframe \n",
    "\n",
    "data=[('Anurag',50000),('Priya',67000),('Ravi',45000),('Stuti',90000),('Ankur',40000)]\n",
    "\n",
    "column=['Name','Salary']\n",
    "\n",
    "spark_df=spark.createDataFrame(data, column)\n",
    "display(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eab4c05-0252-44ea-ba42-c57088655b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Pandas Dataframe\n",
    "import pandas as pd \n",
    "data=[('Anurag',50000),('Priya',67000),('Ravi',45000),('Stuti',90000),('Ankur',40000)]\n",
    "\n",
    "column=['Name','Salary']\n",
    "\n",
    "df=pd.DataFrame(data, columns=column)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cad1dfb2-64bc-421c-a0d5-035f09f50a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b694c9-a0dc-44ff-8f23-547eaf6e6175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Counts in the DF\",spark_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b861eb8f-694e-40de-8f37-367be4d317fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Real-Life Use Case\n",
    "\n",
    "**These DataFrames are used for:**\n",
    "\n",
    "- Ingesting data from files\n",
    "- Transforming at scale\n",
    "- Writing to Delta Tables or Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc65399d-e1c4-4c13-ab3e-b79cd6d3dbef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Add Data Types \n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_typed = spark.createDataFrame(data, schema=schema)\n",
    "df_typed.show()\n",
    "df_typed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4bc0343-df1a-454e-95d2-41afda783554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q: What's the difference between SparkSession and SparkContext?**\n",
    "\n",
    "âœ… Answer:\n",
    "- SparkSession is the unified entry point from Spark 2.0 onwards.\n",
    "- It internally manages SparkContext, SQLContext, and HiveContext.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "113ab13e-8ec0-4241-9e2a-57b3c8e75369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_data = [\n",
    "    {\"Name\": \"David\", \"Salary\": 70000},\n",
    "    {\"Name\": \"Eva\", \"Salary\": 65000}\n",
    "]\n",
    "\n",
    "df_json=spark.createDataFrame(json_data)\n",
    "display(df_json)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Class 1 Handson PySpark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
